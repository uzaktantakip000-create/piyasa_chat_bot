# üöÄ TELEGRAM PIYASA CHAT BOT - PROFESSIONAL UPGRADE ROADMAP

> **Hedef**: Production-ready, enterprise-grade, 50-200 bot √∂l√ßeƒüinde √ßalƒ±≈üan, y√ºksek performanslƒ± sistem
>
> **Hazƒ±rlayan**: Senior Software Architect (Claude Code)
> **Tarih**: 2025-10-27
> **Proje Versiyonu**: v1.5.0 ‚Üí v2.0.0 (Production-Ready Enterprise Edition)

---

## üìä MEVCUT DURUM ANALƒ∞Zƒ∞

### ‚úÖ G√º√ßl√º Y√∂nler
- **Solid Foundation**: FastAPI + React + PostgreSQL/Redis stack
- **Security**: RBAC, MFA, Token encryption (Fernet)
- **Real-time**: WebSocket + REST fallback
- **Rich Features**: Persona/Emotion/Stance system, Semantic dedup
- **Testing**: pytest test suite mevcut
- **Docker**: docker-compose orchestration ready
- **Multi-LLM**: OpenAI, Gemini, Groq desteƒüi

### ‚ö†Ô∏è Kritik ƒ∞yile≈ütirme Alanlarƒ±

#### 1. **Performans Sorunlarƒ±** (P0 - Kritik)
- ‚ùå `behavior_engine.py` monolitik (32k+ tokens)
- ‚ùå Database query optimizasyonu eksik
- ‚ùå Connection pooling suboptimal
- ‚ùå Caching stratejisi yetersiz
- ‚ùå 50+ bot i√ßin stress test sonu√ßlarƒ± belirsiz

#### 2. **Kod Karma≈üƒ±klƒ±ƒüƒ±** (P0 - Kritik)
- ‚ùå `main.py` 1749 satƒ±r (monolitik)
- ‚ùå `backend/` klas√∂r√º yarƒ±m kalmƒ±≈ü (mod√ºlerle≈üme incomplete)
- ‚ùå Type hints eksik/inconsistent
- ‚ùå Error handling standardize deƒüil
- ‚ùå Design patterns eksik

#### 3. **Production Readiness** (P0 - Kritik)
- ‚ùå Monitoring/Observability yok (Prometheus, Grafana, tracing)
- ‚ùå Structured logging yok (JSON logs for parsing)
- ‚ùå Health checks minimal
- ‚ùå Graceful shutdown eksik yerler var
- ‚ùå Rate limiting gran√ºler deƒüil (per-bot rate limiting yok)
- ‚ùå Secret management d√ºzensiz (.env only)

#### 4. **DevOps & Operations** (P1 - Y√ºksek)
- ‚ùå CI/CD pipeline yok
- ‚ùå Kubernetes manifests yok
- ‚ùå Database migrations manuel (Alembic entegrasyonu eksik)
- ‚ùå Automated load testing pipeline yok
- ‚ùå Backup/disaster recovery planƒ± yok

#### 5. **Testing & Quality** (P1 - Y√ºksek)
- ‚ùå Test coverage d√º≈ü√ºk (~40-50% estimated)
- ‚ùå Frontend testleri minimal
- ‚ùå E2E tests yok
- ‚ùå Performance regression tests yok

---

## üéØ ROADMAP OVERVIEW

Roadmap **5 PHASE** olarak yapƒ±landƒ±rƒ±lmƒ±≈ütƒ±r. Her phase baƒüƒ±msƒ±z √ßalƒ±≈üabilir ve incremental value saƒülar.

```
PHASE 1: Performance & Scalability Foundation (2-3 hafta)
    ‚Üì
PHASE 2: Architecture Refactoring & Clean Code (2-3 hafta)
    ‚Üì
PHASE 3: Production-Ready Infrastructure (2 hafta)
    ‚Üì
PHASE 4: DevOps & Automation (1-2 hafta)
    ‚Üì
PHASE 5: Advanced Features & Optimization (1-2 hafta)
```

**Toplam S√ºre**: 8-12 hafta (agresif: 8 hafta, rahat: 12 hafta)

---

## üìÖ PHASE 1: PERFORMANCE & SCALABILITY FOUNDATION
**S√ºre**: 2-3 hafta
**√ñncelik**: P0 (Kritik)
**Hedef**: 50-200 bot'u sorunsuz handle edebilmek

### 1.1 Database Performance Optimization (Hafta 1)

#### Task 1.1.1: Database Index Audit & Optimization
**Neden gerekli**: Mevcut sorgular yava≈ü; Messages tablosunda 50-100 bot ile exponential slowdown riski var.

**Yapƒ±lacaklar**:
```python
# database.py - Mevcut indexleri g√∂zden ge√ßir
# messages tablosunda ek composite indexler:
Index("ix_messages_bot_chat_created", "bot_id", "chat_db_id", "created_at")
Index("ix_messages_chat_telegram_reply", "chat_db_id", "telegram_message_id", "reply_to_message_id")

# bot_stances, bot_holdings i√ßin partial indexes:
Index("ix_stances_active_cooldown", "bot_id", "topic", postgresql_where=(cooldown_until > datetime.now()))

# ANALYZE sorgu planlarƒ±nƒ± incele:
# SELECT * FROM messages WHERE chat_db_id = ? ORDER BY created_at DESC LIMIT 10
# -> Index scan kullanƒ±ldƒ±ƒüƒ±ndan emin ol
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Query execution time < 50ms for typical history fetches (10 messages)
- [ ] Database CPU usage < 30% at 100 bot load

**Dok√ºman**: `docs/database_optimization.md`

---

#### Task 1.1.2: Connection Pool Tuning
**Neden gerekli**: Varsayƒ±lan pool_size=20, max_overflow=40. 50+ bot i√ßin yetersiz.

**Yapƒ±lacaklar**:
```python
# database.py
# Pool size form√ºl√º: (bot_count * 2) + API_workers
# 100 bot + 4 API worker = ~204 connection
pool_size = int(os.getenv("DB_POOL_SIZE", "50"))
max_overflow = int(os.getenv("DB_MAX_OVERFLOW", "100"))
pool_timeout = int(os.getenv("DB_POOL_TIMEOUT", "30"))
pool_recycle = int(os.getenv("DB_POOL_RECYCLE", "3600"))

engine = create_engine(
    DATABASE_URL,
    pool_size=pool_size,
    max_overflow=max_overflow,
    pool_timeout=pool_timeout,
    pool_recycle=pool_recycle,
    pool_pre_ping=True,
    echo_pool=True if LOG_LEVEL == "DEBUG" else False
)
```

**Test**:
```bash
# Load test: 100 bot simultaneously writing messages
python scripts/load_test_db.py --bots 100 --duration 60
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Connection pool exhaustion yok (max_overflow limit'e ula≈ümamalƒ±)
- [ ] Average connection wait time < 10ms

---

#### Task 1.1.3: Query Optimization & N+1 Problem
**Neden gerekli**: `behavior_engine.py` i√ßinde bazƒ± sorgular N+1 pattern (loop i√ßinde query).

**Sorunlu kod √∂rneƒüi**:
```python
# ‚ùå BAD: N+1 query problem
for bot in bots:
    stances = db.query(BotStance).filter(BotStance.bot_id == bot.id).all()
    holdings = db.query(BotHolding).filter(BotHolding.bot_id == bot.id).all()
```

**√á√∂z√ºm**:
```python
# ‚úÖ GOOD: Eager loading with joinedload
from sqlalchemy.orm import joinedload

bots = db.query(Bot).options(
    joinedload(Bot.stances),
    joinedload(Bot.holdings)
).filter(Bot.is_enabled == True).all()
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Eliminate all N+1 queries (SQLAlchemy echo ile doƒürula)
- [ ] Total query count < 5 per message generation

---

### 1.2 Caching Strategy Implementation (Hafta 1-2)

#### Task 1.2.1: Multi-Layer Cache Architecture
**Neden gerekli**: LLM calls, database queries, news fetching √ßok yava≈ü; cache ile 10-100x speedup.

**Yapƒ±lacaklar**:
```
Layer 1: In-Memory Cache (TTL: 60s) - Hot data
    ‚Üì
Layer 2: Redis Cache (TTL: 5min) - Shared across workers
    ‚Üì
Layer 3: Database - Cold data
```

**Implementation**:
```python
# caching.py - Yeni mod√ºl olu≈ütur
from functools import lru_cache
import redis
import json
from typing import Any, Optional
import hashlib

class CacheManager:
    def __init__(self, redis_client: Optional[redis.Redis]):
        self.redis = redis_client
        self._local_cache = {}  # In-memory dict

    def get(self, key: str, level: str = "redis") -> Optional[Any]:
        # Level 1: Local
        if level == "local" and key in self._local_cache:
            return self._local_cache[key]

        # Level 2: Redis
        if self.redis:
            cached = self.redis.get(key)
            if cached:
                return json.loads(cached)

        return None

    def set(self, key: str, value: Any, ttl: int = 300, level: str = "redis"):
        # Level 1: Local
        if level == "local":
            self._local_cache[key] = value

        # Level 2: Redis
        if self.redis:
            self.redis.setex(key, ttl, json.dumps(value))

    def invalidate(self, pattern: str):
        # Clear matching keys
        if self.redis:
            for key in self.redis.scan_iter(pattern):
                self.redis.delete(key)

# behavior_engine.py i√ßinde kullan
cache_manager = CacheManager(get_redis())

# Cache persona/emotion/stances (5 dakika TTL)
cache_key = f"bot:{bot.id}:profile"
cached_profile = cache_manager.get(cache_key)
if not cached_profile:
    cached_profile = {
        "persona": bot.persona_profile,
        "emotion": bot.emotion_profile,
        "stances": [s.dict() for s in bot.stances],
        "holdings": [h.dict() for h in bot.holdings]
    }
    cache_manager.set(cache_key, cached_profile, ttl=300)
```

**Cache Invalidation Strategy**:
```python
# main.py - Config update olduƒüunda cache'i invalidate et
@app.put("/bots/{bot_id}/persona")
def put_persona(...):
    # ... update DB ...
    cache_manager.invalidate(f"bot:{bot_id}:*")
    publish_config_update(...)
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Cache hit rate > 80% for bot profiles
- [ ] Average message generation time < 2s (was ~5s)

---

#### Task 1.2.2: Message History Caching
**Neden gerekli**: Her message generation i√ßin son 10-20 mesaj DB'den √ßekiliyor; cache ile 50x faster.

**Implementation**:
```python
# message_cache.py - zaten var, optimize et
class MessageCache:
    def __init__(self, redis_client: redis.Redis, ttl: int = 300):
        self.redis = redis_client
        self.ttl = ttl

    def get_recent_messages(self, chat_id: int, limit: int = 20) -> List[Dict]:
        cache_key = f"chat:{chat_id}:history:{limit}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        return None

    def set_recent_messages(self, chat_id: int, messages: List[Dict], limit: int = 20):
        cache_key = f"chat:{chat_id}:history:{limit}"
        self.redis.setex(cache_key, self.ttl, json.dumps(messages))

    def invalidate_chat(self, chat_id: int):
        # Yeni mesaj eklendiƒüinde cache'i invalidate et
        for key in self.redis.scan_iter(f"chat:{chat_id}:history:*"):
            self.redis.delete(key)

# behavior_engine.py i√ßinde kullan
msg_cache = MessageCache(redis_client)

# Cache'ten oku
history = msg_cache.get_recent_messages(chat.id, limit=20)
if not history:
    # DB'den √ßek
    history = db.query(Message).filter(...).limit(20).all()
    msg_cache.set_recent_messages(chat.id, [serialize(m) for m in history])
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] History fetch time < 5ms (was ~50-100ms)
- [ ] Cache hit rate > 90%

---

### 1.3 Async & Concurrency Optimization (Hafta 2)

#### Task 1.3.1: Database Async Queries (SQLAlchemy AsyncEngine)
**Neden gerekli**: Blocking I/O operations worker threads'i block ediyor; async ile concurrency artacak.

**Yapƒ±lacaklar**:
```python
# database_async.py - Yeni mod√ºl olu≈ütur
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker

async_engine = create_async_engine(
    DATABASE_URL.replace("postgresql+psycopg", "postgresql+asyncpg"),
    pool_size=50,
    max_overflow=100,
    echo=False
)

AsyncSessionLocal = async_sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False
)

async def get_async_db():
    async with AsyncSessionLocal() as session:
        yield session

# behavior_engine.py i√ßinde async queries
async def fetch_bot_stances(bot_id: int, db: AsyncSession):
    result = await db.execute(
        select(BotStance).where(BotStance.bot_id == bot_id)
    )
    return result.scalars().all()
```

**Migration Plan**:
1. Week 2: Async engine setup + migrate read-only queries
2. Week 3: Migrate write queries + test concurrent load

**√ñl√ß√ºlebilir Hedef**:
- [ ] Concurrent message generation: 20+ messages/sec (was ~5/sec)
- [ ] Worker CPU utilization > 70% (was ~40% due to blocking I/O)

---

#### Task 1.3.2: LLM Call Batching & Parallel Execution
**Neden gerekli**: Sequential LLM calls bottleneck; parallel execution ile 3-5x speedup.

**Implementation**:
```python
# llm_client.py - Batch API support ekle
import asyncio

async def generate_messages_batch(prompts: List[str], model: str) -> List[str]:
    """Generate multiple messages in parallel"""
    tasks = [
        asyncio.create_task(generate_message_async(prompt, model))
        for prompt in prompts
    ]
    return await asyncio.gather(*tasks)

# behavior_engine.py i√ßinde paralel generation
async def tick_batch(bot_ids: List[int], chat_id: int):
    """Generate messages for multiple bots in parallel"""
    prompts = [build_prompt(bot_id, chat_id) for bot_id in bot_ids]
    responses = await generate_messages_batch(prompts, model="gpt-4o-mini")

    # Save all messages to DB in batch
    await db.execute(insert(Message).values([
        {"bot_id": bot_id, "text": response, ...}
        for bot_id, response in zip(bot_ids, responses)
    ]))
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Batch generation latency < 3s for 10 bots (was ~15s sequential)
- [ ] OpenAI API rate limit efficiency > 90%

---

### 1.4 Worker Scaling & Load Balancing (Hafta 2-3)

#### Task 1.4.1: Multi-Worker Architecture
**Neden gerekli**: Tek worker 50+ bot i√ßin yetersiz; horizontal scaling gerekli.

**Implementation**:
```yaml
# docker-compose.yml - Worker replicas ekle
services:
  worker:
    deploy:
      replicas: 4  # 4 worker instance
    environment:
      - WORKER_ID=${WORKER_ID:-0}
      - TOTAL_WORKERS=${TOTAL_WORKERS:-4}

# worker.py - Worker ID based task distribution
WORKER_ID = int(os.getenv("WORKER_ID", "0"))
TOTAL_WORKERS = int(os.getenv("TOTAL_WORKERS", "1"))

async def should_process_bot(bot_id: int) -> bool:
    """Consistent hashing for bot assignment"""
    return (bot_id % TOTAL_WORKERS) == WORKER_ID
```

**Redis Queue for Work Distribution**:
```python
# message_queue.py - Priority queue system (zaten var, optimize et)
class PriorityQueue:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client

    def enqueue(self, task: Dict, priority: str = "normal"):
        """Add task to queue"""
        queue_key = f"queue:{priority}"
        self.redis.lpush(queue_key, json.dumps(task))

    def dequeue(self, priorities: List[str] = ["high", "normal", "low"]) -> Optional[Dict]:
        """Dequeue task (FIFO within priority)"""
        for priority in priorities:
            queue_key = f"queue:{priority}"
            task = self.redis.rpop(queue_key)
            if task:
                return json.loads(task)
        return None

# Worker'lar queue'dan task alƒ±r (load balanced automatically)
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Linear scaling: 4 workers = 4x throughput
- [ ] Load distribution variance < 15% across workers

---

#### Task 1.4.2: Circuit Breaker & Retry Policy
**Neden gerekli**: Telegram/OpenAI API failures cascade etmemeli; graceful degradation.

**Implementation**:
```python
# circuit_breaker.py - Yeni mod√ºl olu≈ütur
from enum import Enum
import time

class CircuitState(Enum):
    CLOSED = "closed"       # Normal operation
    OPEN = "open"           # Failures detected, block requests
    HALF_OPEN = "half_open" # Testing if service recovered

class CircuitBreaker:
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker OPEN - service unavailable")

        try:
            result = func(*args, **kwargs)
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.CLOSED
                self.failures = 0
            return result
        except Exception as e:
            self.failures += 1
            self.last_failure_time = time.time()
            if self.failures >= self.failure_threshold:
                self.state = CircuitState.OPEN
            raise

# telegram_client.py i√ßinde kullan
telegram_circuit = CircuitBreaker(failure_threshold=5, timeout=60)

def send_message_safe(bot_token, chat_id, text):
    return telegram_circuit.call(send_message, bot_token, chat_id, text)
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] System uptime > 99.5% even with external API failures
- [ ] Cascade failure prevention (isolated failures don't bring down whole system)

---

### 1.5 Performance Testing & Benchmarking (Hafta 3)

#### Task 1.5.1: Load Testing Framework
**Neden gerekli**: 50-200 bot production load simulation; bottleneck detection.

**Implementation**:
```python
# tests/load_test.py - Kapsamlƒ± load test suite
import asyncio
import aiohttp
from locust import HttpUser, task, between

class BotSimulationUser(HttpUser):
    wait_time = between(1, 3)

    @task
    def send_message(self):
        # Simulate bot sending message
        self.client.post("/control/start")

# Run with:
# locust -f tests/load_test.py --users 200 --spawn-rate 10
```

**Metrics to Track**:
```python
# metrics_collector.py - Prometheus metrics
from prometheus_client import Counter, Histogram, Gauge

message_generation_duration = Histogram(
    "message_generation_seconds",
    "Time to generate a message",
    buckets=[0.5, 1, 2, 5, 10, 30]
)

active_bot_count = Gauge(
    "active_bots_total",
    "Number of active bots"
)

telegram_api_errors = Counter(
    "telegram_api_errors_total",
    "Telegram API error count",
    ["error_type"]
)
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] 50 bot @ 10 msg/hour: avg latency < 2s, p99 < 5s
- [ ] 100 bot @ 10 msg/hour: avg latency < 3s, p99 < 8s
- [ ] 200 bot @ 5 msg/hour: avg latency < 5s, p99 < 15s
- [ ] Zero message loss under load

**Deliverables**:
- [ ] Load test report: `docs/load_test_report_v2.md`
- [ ] Performance dashboard: Grafana JSON export

---

## üìÖ PHASE 2: ARCHITECTURE REFACTORING & CLEAN CODE
**S√ºre**: 2-3 hafta
**√ñncelik**: P0 (Kritik)
**Hedef**: Maintainable, extensible, clean codebase

### 2.1 Behavior Engine Modularization (Hafta 4)

#### Task 2.1.1: behavior_engine.py Refactoring
**Neden gerekli**: 32k+ tokens, anla≈üƒ±lmasƒ± zor, modify etmek riskli.

**Target Architecture**:
```
behavior_engine/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ core.py                  # BehaviorEngine class (orchestrator)
‚îú‚îÄ‚îÄ message_generator.py     # Message generation logic
‚îú‚îÄ‚îÄ prompt_builder.py        # Prompt construction
‚îú‚îÄ‚îÄ topic_selector.py        # Topic scoring & selection
‚îú‚îÄ‚îÄ persona_manager.py       # Persona refresh & management
‚îú‚îÄ‚îÄ reaction_planner.py      # Emotion-driven reaction planning
‚îú‚îÄ‚îÄ consistency_guard.py     # Stance validation
‚îú‚îÄ‚îÄ deduplication.py         # Duplicate detection & paraphrasing
‚îî‚îÄ‚îÄ typing_simulator.py      # Typing action simulation
```

**Refactoring Strategy**:
```python
# behavior_engine/core.py - Simplified orchestrator
class BehaviorEngine:
    def __init__(self, db: Session, redis_client: redis.Redis):
        self.db = db
        self.redis = redis_client
        self.message_gen = MessageGenerator(db)
        self.prompt_builder = PromptBuilder()
        self.topic_selector = TopicSelector()
        # ... inject dependencies

    async def tick_once(self):
        """Main engine loop - cleaner orchestration"""
        # 1. Pick chat & bot
        chat, bot = await self._select_chat_and_bot()

        # 2. Select topic
        topic = await self.topic_selector.select(chat, bot)

        # 3. Build prompt
        prompt = await self.prompt_builder.build(bot, chat, topic)

        # 4. Generate message
        message = await self.message_gen.generate(prompt)

        # 5. Validate consistency
        message = await self.consistency_guard.validate(message, bot)

        # 6. Check deduplication
        message = await self.deduplication.check(message, bot)

        # 7. Send to Telegram
        await self._send_message(bot, chat, message)

# behavior_engine/message_generator.py - Single responsibility
class MessageGenerator:
    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    async def generate(self, prompt: str, model: str = "gpt-4o-mini") -> str:
        """Generate message from prompt"""
        response = await self.llm.chat_completion(prompt, model)
        return self._clean_response(response)

    def _clean_response(self, text: str) -> str:
        """Remove artifacts, fix formatting"""
        # ...
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Each module < 300 lines
- [ ] Cyclomatic complexity < 10 per function
- [ ] Test coverage > 80% per module

---

#### Task 2.1.2: Design Patterns Implementation
**Neden gerekli**: Hard-coded dependencies, tight coupling; DI (Dependency Injection) ile loosely coupled.

**Patterns to Implement**:

1. **Strategy Pattern** (Topic Selection):
```python
# topic_selector.py
from abc import ABC, abstractmethod

class TopicSelectionStrategy(ABC):
    @abstractmethod
    def score(self, topic: str, context: Dict) -> float:
        pass

class KeywordBasedStrategy(TopicSelectionStrategy):
    def score(self, topic: str, context: Dict) -> float:
        # Keyword matching logic
        pass

class EmbeddingBasedStrategy(TopicSelectionStrategy):
    def score(self, topic: str, context: Dict) -> float:
        # Semantic similarity logic
        pass

class TopicSelector:
    def __init__(self, strategy: TopicSelectionStrategy):
        self.strategy = strategy

    def select(self, topics: List[str], context: Dict) -> str:
        scores = [(topic, self.strategy.score(topic, context)) for topic in topics]
        return max(scores, key=lambda x: x[1])[0]
```

2. **Factory Pattern** (LLM Client):
```python
# llm_client_factory.py
class LLMClientFactory:
    @staticmethod
    def create(provider: str) -> LLMClient:
        if provider == "openai":
            return OpenAIClient(api_key=os.getenv("OPENAI_API_KEY"))
        elif provider == "gemini":
            return GeminiClient(api_key=os.getenv("GEMINI_API_KEY"))
        elif provider == "groq":
            return GroqClient(api_key=os.getenv("GROQ_API_KEY"))
        else:
            raise ValueError(f"Unknown provider: {provider}")

# Config'ten provider al
llm_client = LLMClientFactory.create(os.getenv("LLM_PROVIDER", "openai"))
```

3. **Observer Pattern** (Config Updates):
```python
# config_observer.py
class ConfigObserver(ABC):
    @abstractmethod
    def on_config_update(self, update: Dict):
        pass

class BehaviorEngineObserver(ConfigObserver):
    def __init__(self, engine: BehaviorEngine):
        self.engine = engine

    def on_config_update(self, update: Dict):
        if update.get("type") == "bot_updated":
            self.engine.invalidate_bot_cache(update["bot_id"])

# Redis subscriber
def subscribe_to_config_updates():
    pubsub = redis_client.pubsub()
    pubsub.subscribe("config_updates")
    for message in pubsub.listen():
        for observer in observers:
            observer.on_config_update(json.loads(message["data"]))
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Dependency injection coverage > 90%
- [ ] Coupling metrics (afferent/efferent coupling) improved

---

### 2.2 API Modularization (Hafta 4-5)

#### Task 2.2.1: main.py Route Migration
**Neden gerekli**: 1749 satƒ±r monolitik API; `backend/` klas√∂r√ºne mod√ºler route yapƒ±sƒ±.

**Target Structure**:
```
backend/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py        # Shared dependencies (auth, db)
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ auth.py            # ‚úÖ DONE
‚îÇ       ‚îú‚îÄ‚îÄ bots.py            # TODO: /bots CRUD
‚îÇ       ‚îú‚îÄ‚îÄ chats.py           # TODO: /chats CRUD
‚îÇ       ‚îú‚îÄ‚îÄ settings.py        # TODO: /settings
‚îÇ       ‚îú‚îÄ‚îÄ control.py         # TODO: /control/start|stop|scale
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py         # TODO: /metrics, /queue/stats
‚îÇ       ‚îú‚îÄ‚îÄ websockets.py      # TODO: /ws/dashboard
‚îÇ       ‚îú‚îÄ‚îÄ logs.py            # TODO: /logs
‚îÇ       ‚îú‚îÄ‚îÄ system_checks.py   # TODO: /system/checks
‚îÇ       ‚îú‚îÄ‚îÄ personas.py        # TODO: /bots/{id}/persona|emotion
‚îÇ       ‚îú‚îÄ‚îÄ stances.py         # TODO: /bots/{id}/stances, /stances/{id}
‚îÇ       ‚îú‚îÄ‚îÄ holdings.py        # TODO: /bots/{id}/holdings, /holdings/{id}
‚îÇ       ‚îú‚îÄ‚îÄ wizard.py          # TODO: /wizard/setup|example
‚îÇ       ‚îî‚îÄ‚îÄ webhooks.py        # TODO: /webhook/telegram/{token}
‚îú‚îÄ‚îÄ services/                  # Business logic layer
‚îÇ   ‚îú‚îÄ‚îÄ bot_service.py
‚îÇ   ‚îú‚îÄ‚îÄ chat_service.py
‚îÇ   ‚îú‚îÄ‚îÄ message_service.py
‚îÇ   ‚îî‚îÄ‚îÄ system_check_service.py
‚îî‚îÄ‚îÄ schemas/                   # Pydantic schemas (zaten var, organize et)
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ bot_schemas.py
    ‚îú‚îÄ‚îÄ chat_schemas.py
    ‚îî‚îÄ‚îÄ ...
```

**Migration Example**:
```python
# backend/api/routes/bots.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from backend.api.dependencies import get_db, operator_dependencies
from backend.services.bot_service import BotService
from backend.schemas.bot_schemas import BotCreate, BotUpdate, BotResponse

router = APIRouter(prefix="/bots", tags=["Bots"])

@router.post("", response_model=BotResponse, status_code=201, dependencies=operator_dependencies)
async def create_bot(bot: BotCreate, db: Session = Depends(get_db)):
    service = BotService(db)
    return await service.create_bot(bot)

@router.get("", response_model=List[BotResponse])
async def list_bots(db: Session = Depends(get_db)):
    service = BotService(db)
    return await service.list_bots()

# backend/services/bot_service.py - Business logic
class BotService:
    def __init__(self, db: Session):
        self.db = db

    async def create_bot(self, bot_data: BotCreate) -> Bot:
        """Create bot with validation & encryption"""
        db_bot = Bot(
            name=bot_data.name,
            token=bot_data.token,  # Auto-encrypted via setter
            username=bot_data.username,
            is_enabled=bot_data.is_enabled,
            # ...
        )
        self.db.add(db_bot)
        self.db.commit()
        self.db.refresh(db_bot)

        # Publish config update
        publish_config_update(get_redis(), {"type": "bot_added", "bot_id": db_bot.id})

        return db_bot

# main.py - Simplified to route registration only
from backend.api.routes import auth, bots, chats, settings, control, metrics, logs

app = FastAPI()
app.include_router(auth.router)
app.include_router(bots.router)
app.include_router(chats.router)
# ... include all routers
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] main.py < 200 lines (route registration only)
- [ ] Each route module < 300 lines
- [ ] Business logic in services/ layer (separation of concerns)

---

#### Task 2.2.2: Type Hints & Pydantic Validation
**Neden gerekli**: Type safety eksik; runtime errors √∂nlenebilir.

**Implementation**:
```python
# backend/services/bot_service.py - Full type hints
from typing import List, Optional
from datetime import datetime

class BotService:
    def __init__(self, db: Session) -> None:
        self.db = db

    async def get_bot(self, bot_id: int) -> Optional[Bot]:
        """Get bot by ID"""
        return self.db.query(Bot).filter(Bot.id == bot_id).first()

    async def list_bots(
        self,
        enabled_only: bool = False,
        limit: int = 100,
        offset: int = 0
    ) -> List[Bot]:
        """List bots with pagination"""
        query = self.db.query(Bot)
        if enabled_only:
            query = query.filter(Bot.is_enabled == True)
        return query.limit(limit).offset(offset).all()

# Pydantic strict validation
from pydantic import BaseModel, Field, validator

class BotCreate(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    token: str = Field(..., min_length=10, regex=r"^\d+:[A-Za-z0-9_-]+$")
    username: str = Field(..., regex=r"^@?[a-zA-Z0-9_]{5,32}$")
    is_enabled: bool = True

    @validator("username")
    def normalize_username(cls, v):
        """Ensure username starts with @"""
        return v if v.startswith("@") else f"@{v}"

# Type checking with mypy
# Run: mypy backend/ --strict
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Type hint coverage > 95%
- [ ] Zero mypy errors in strict mode

---

### 2.3 Error Handling Standardization (Hafta 5)

#### Task 2.3.1: Custom Exception Hierarchy
**Neden gerekli**: Generic exceptions debugging zorla≈ütƒ±rƒ±yor; domain-specific exceptions.

**Implementation**:
```python
# backend/exceptions.py - Centralized exception hierarchy
class PiyasaChatBotException(Exception):
    """Base exception for all app errors"""
    def __init__(self, message: str, code: str, details: Optional[Dict] = None):
        self.message = message
        self.code = code
        self.details = details or {}
        super().__init__(message)

class BotNotFoundException(PiyasaChatBotException):
    def __init__(self, bot_id: int):
        super().__init__(
            message=f"Bot {bot_id} not found",
            code="BOT_NOT_FOUND",
            details={"bot_id": bot_id}
        )

class TokenEncryptionError(PiyasaChatBotException):
    def __init__(self, reason: str):
        super().__init__(
            message=f"Token encryption failed: {reason}",
            code="TOKEN_ENCRYPTION_ERROR",
            details={"reason": reason}
        )

class TelegramAPIError(PiyasaChatBotException):
    def __init__(self, status_code: int, response: str):
        super().__init__(
            message=f"Telegram API error: {status_code}",
            code="TELEGRAM_API_ERROR",
            details={"status_code": status_code, "response": response}
        )

class LLMProviderError(PiyasaChatBotException):
    def __init__(self, provider: str, reason: str):
        super().__init__(
            message=f"LLM provider {provider} error: {reason}",
            code="LLM_PROVIDER_ERROR",
            details={"provider": provider, "reason": reason}
        )

# FastAPI exception handlers
from fastapi import Request
from fastapi.responses import JSONResponse

@app.exception_handler(PiyasaChatBotException)
async def custom_exception_handler(request: Request, exc: PiyasaChatBotException):
    return JSONResponse(
        status_code=400,  # Or map code to status
        content={
            "error": {
                "message": exc.message,
                "code": exc.code,
                "details": exc.details
            }
        }
    )

# Usage
def get_bot_or_fail(bot_id: int, db: Session) -> Bot:
    bot = db.query(Bot).filter(Bot.id == bot_id).first()
    if not bot:
        raise BotNotFoundException(bot_id)
    return bot
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] All exceptions inherit from base exception
- [ ] Exception coverage > 90% of error paths

---

#### Task 2.3.2: Structured Error Logging
**Neden gerekli**: Plain text logs parseable deƒüil; JSON structured logs monitoring i√ßin gerekli.

**Implementation**:
```python
# backend/logging_config.py - Structured JSON logging
import logging
import json
from datetime import datetime
from typing import Any, Dict

class JSONFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_data: Dict[str, Any] = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }

        # Add exception info if present
        if record.exc_info:
            log_data["exception"] = {
                "type": record.exc_info[0].__name__,
                "message": str(record.exc_info[1]),
                "traceback": self.formatException(record.exc_info)
            }

        # Add custom fields from extra
        if hasattr(record, "extra_fields"):
            log_data.update(record.extra_fields)

        return json.dumps(log_data)

# Configure logging
def setup_logging():
    handler = logging.StreamHandler()
    handler.setFormatter(JSONFormatter())

    root_logger = logging.getLogger()
    root_logger.addHandler(handler)
    root_logger.setLevel(os.getenv("LOG_LEVEL", "INFO"))

# Usage with structured fields
logger = logging.getLogger(__name__)
logger.info(
    "Message generated successfully",
    extra={
        "extra_fields": {
            "bot_id": 123,
            "chat_id": 456,
            "message_length": 150,
            "generation_time_ms": 234
        }
    }
)
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] 100% JSON structured logs
- [ ] Logs parseable by Elasticsearch/CloudWatch

---

## üìÖ PHASE 3: PRODUCTION-READY INFRASTRUCTURE
**S√ºre**: 2 hafta
**√ñncelik**: P0 (Kritik)
**Hedef**: Enterprise monitoring, reliability, security

### 3.1 Monitoring & Observability (Hafta 6)

#### Task 3.1.1: Prometheus Metrics Exporter
**Neden gerekli**: Blind production deployment; real-time metrics i√ßin Prometheus.

**Implementation**:
```python
# backend/metrics/prometheus_exporter.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest, REGISTRY
from fastapi import Response

# Define metrics
http_requests_total = Counter(
    "http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status"]
)

http_request_duration_seconds = Histogram(
    "http_request_duration_seconds",
    "HTTP request latency",
    ["method", "endpoint"],
    buckets=[0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]
)

message_generation_total = Counter(
    "message_generation_total",
    "Total messages generated",
    ["bot_id", "status"]  # status: success|failed
)

message_generation_duration_seconds = Histogram(
    "message_generation_duration_seconds",
    "Message generation latency",
    buckets=[0.5, 1, 2, 5, 10, 30, 60]
)

active_bots_gauge = Gauge(
    "active_bots_total",
    "Number of active bots"
)

database_connections_gauge = Gauge(
    "database_connections_active",
    "Active database connections"
)

redis_operations_total = Counter(
    "redis_operations_total",
    "Redis operations count",
    ["operation", "status"]  # operation: get|set|del, status: success|error
)

telegram_api_calls_total = Counter(
    "telegram_api_calls_total",
    "Telegram API calls",
    ["method", "status"]  # method: sendMessage|setReaction, status: 200|429|500
)

llm_api_calls_total = Counter(
    "llm_api_calls_total",
    "LLM API calls",
    ["provider", "model", "status"]
)

llm_token_usage_total = Counter(
    "llm_token_usage_total",
    "Total LLM tokens consumed",
    ["provider", "model", "type"]  # type: prompt|completion
)

# Middleware for HTTP metrics
from starlette.middleware.base import BaseHTTPMiddleware
import time

class PrometheusMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        start_time = time.time()
        response = await call_next(request)
        duration = time.time() - start_time

        http_requests_total.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).inc()

        http_request_duration_seconds.labels(
            method=request.method,
            endpoint=request.url.path
        ).observe(duration)

        return response

# Add to FastAPI
app.add_middleware(PrometheusMiddleware)

# Metrics endpoint
@app.get("/metrics")
def metrics():
    return Response(content=generate_latest(REGISTRY), media_type="text/plain")

# Instrument behavior engine
class BehaviorEngine:
    async def generate_message(self, bot, chat):
        start_time = time.time()
        try:
            message = await self._generate(bot, chat)

            message_generation_total.labels(
                bot_id=bot.id,
                status="success"
            ).inc()

            message_generation_duration_seconds.observe(time.time() - start_time)

            return message
        except Exception as e:
            message_generation_total.labels(
                bot_id=bot.id,
                status="failed"
            ).inc()
            raise
```

**Prometheus Configuration**:
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'piyasa-chatbot-api'
    static_configs:
      - targets: ['api:8000']
    scrape_interval: 15s

  - job_name: 'piyasa-chatbot-worker'
    static_configs:
      - targets: ['worker:8001']  # Expose metrics on worker too
    scrape_interval: 15s
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Metrics coverage: 20+ business metrics
- [ ] Prometheus scrape success rate > 99%

---

#### Task 3.1.2: Grafana Dashboards
**Neden gerekli**: Metrics visualization; real-time operational insights.

**Implementation**:
```json
// grafana/dashboards/piyasa_chatbot_overview.json
{
  "dashboard": {
    "title": "Piyasa ChatBot - Production Overview",
    "panels": [
      {
        "title": "Message Generation Rate",
        "targets": [{
          "expr": "rate(message_generation_total[5m])"
        }]
      },
      {
        "title": "Message Generation Latency (p50, p95, p99)",
        "targets": [
          {"expr": "histogram_quantile(0.50, message_generation_duration_seconds)"},
          {"expr": "histogram_quantile(0.95, message_generation_duration_seconds)"},
          {"expr": "histogram_quantile(0.99, message_generation_duration_seconds)"}
        ]
      },
      {
        "title": "Active Bots",
        "targets": [{"expr": "active_bots_total"}]
      },
      {
        "title": "Telegram API Error Rate",
        "targets": [{
          "expr": "rate(telegram_api_calls_total{status!=\"200\"}[5m])"
        }]
      },
      {
        "title": "LLM Token Usage (Daily)",
        "targets": [{
          "expr": "increase(llm_token_usage_total[24h])"
        }]
      },
      {
        "title": "Database Connection Pool",
        "targets": [{
          "expr": "database_connections_active"
        }]
      }
    ]
  }
}
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] 3+ Grafana dashboards (Overview, Performance, Errors)
- [ ] Dashboard refresh rate: 5s

---

#### Task 3.1.3: Distributed Tracing (OpenTelemetry)
**Neden gerekli**: Multi-service latency debugging; distributed tracing.

**Implementation**:
```python
# backend/tracing.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor

def setup_tracing(app):
    # Configure tracer
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)

    # Jaeger exporter
    jaeger_exporter = JaegerExporter(
        agent_host_name=os.getenv("JAEGER_AGENT_HOST", "localhost"),
        agent_port=int(os.getenv("JAEGER_AGENT_PORT", "6831"))
    )

    tracer_provider.add_span_processor(BatchSpanProcessor(jaeger_exporter))

    # Auto-instrument FastAPI
    FastAPIInstrumentor.instrument_app(app)

    # Auto-instrument SQLAlchemy
    SQLAlchemyInstrumentor().instrument(engine=engine)

# Manual span creation for business logic
tracer = trace.get_tracer(__name__)

async def generate_message(bot, chat):
    with tracer.start_as_current_span("generate_message") as span:
        span.set_attribute("bot.id", bot.id)
        span.set_attribute("chat.id", chat.id)

        # Sub-span for LLM call
        with tracer.start_as_current_span("llm_api_call") as llm_span:
            response = await llm_client.generate(prompt)
            llm_span.set_attribute("tokens.prompt", response.usage.prompt_tokens)
            llm_span.set_attribute("tokens.completion", response.usage.completion_tokens)

        return response.text
```

**docker-compose.yml - Add Jaeger**:
```yaml
services:
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # Jaeger UI
      - "6831:6831/udp"  # Jaeger agent
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Trace coverage > 80% of critical paths
- [ ] E2E trace latency breakdown visible in Jaeger UI

---

### 3.2 Health Checks & Reliability (Hafta 6)

#### Task 3.2.1: Comprehensive Health Checks
**Neden gerekli**: Load balancer health checks; Kubernetes readiness/liveness probes.

**Implementation**:
```python
# backend/api/routes/health.py
from fastapi import APIRouter, status
from pydantic import BaseModel
from typing import Dict, List
import asyncio

router = APIRouter(prefix="/health", tags=["Health"])

class HealthCheckResult(BaseModel):
    status: str  # healthy|degraded|unhealthy
    checks: Dict[str, Dict[str, any]]
    version: str
    uptime_seconds: float

async def check_database() -> Dict:
    """Check database connectivity"""
    try:
        db = SessionLocal()
        db.execute("SELECT 1")
        db.close()
        return {"status": "healthy", "latency_ms": 5}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

async def check_redis() -> Dict:
    """Check Redis connectivity"""
    try:
        r = get_redis()
        if not r:
            return {"status": "degraded", "message": "Redis not configured"}
        r.ping()
        return {"status": "healthy"}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

async def check_llm_provider() -> Dict:
    """Check LLM provider availability"""
    try:
        # Quick health check (don't waste tokens)
        llm_client = LLMClientFactory.create(os.getenv("LLM_PROVIDER"))
        # Optionally: call a lightweight endpoint
        return {"status": "healthy", "provider": os.getenv("LLM_PROVIDER")}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

async def check_telegram_api() -> Dict:
    """Check Telegram API availability"""
    try:
        response = await httpx.get("https://api.telegram.org", timeout=5)
        if response.status_code == 200:
            return {"status": "healthy"}
        return {"status": "degraded", "status_code": response.status_code}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

@router.get("/live", status_code=status.HTTP_200_OK)
async def liveness():
    """Kubernetes liveness probe - is process alive?"""
    return {"status": "alive"}

@router.get("/ready", status_code=status.HTTP_200_OK)
async def readiness():
    """Kubernetes readiness probe - can handle traffic?"""
    # Check critical dependencies
    checks = await asyncio.gather(
        check_database(),
        check_redis(),
        return_exceptions=True
    )

    if all(c.get("status") == "healthy" for c in checks):
        return {"status": "ready"}

    # If any critical check fails, return 503
    return JSONResponse(
        status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
        content={"status": "not_ready", "checks": checks}
    )

@router.get("/health", response_model=HealthCheckResult)
async def health():
    """Comprehensive health check"""
    start_time = time.time()

    checks = await asyncio.gather(
        check_database(),
        check_redis(),
        check_llm_provider(),
        check_telegram_api(),
        return_exceptions=True
    )

    check_results = {
        "database": checks[0],
        "redis": checks[1],
        "llm_provider": checks[2],
        "telegram_api": checks[3]
    }

    # Determine overall status
    statuses = [c.get("status") for c in check_results.values()]
    if all(s == "healthy" for s in statuses):
        overall_status = "healthy"
    elif any(s == "unhealthy" for s in statuses):
        overall_status = "unhealthy"
    else:
        overall_status = "degraded"

    return HealthCheckResult(
        status=overall_status,
        checks=check_results,
        version=app.version,
        uptime_seconds=time.time() - start_time
    )
```

**Kubernetes Integration**:
```yaml
# k8s/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: api
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Health check latency < 100ms
- [ ] K8s probes configured for all services

---

#### Task 3.2.2: Graceful Shutdown
**Neden gerekli**: In-flight requests dropped during deployment; graceful shutdown prevents data loss.

**Implementation**:
```python
# main.py - Graceful shutdown handler
import signal
import asyncio

shutdown_event = asyncio.Event()

async def graceful_shutdown():
    """Handle graceful shutdown"""
    logger.info("Graceful shutdown initiated...")

    # 1. Stop accepting new requests (mark as not ready)
    app.state.accepting_requests = False

    # 2. Wait for in-flight requests to complete (max 30s)
    await asyncio.sleep(2)  # Give load balancer time to deregister

    # 3. Close background tasks
    if hasattr(app.state, "background_tasks"):
        for task in app.state.background_tasks:
            task.cancel()

    # 4. Close database connections
    engine.dispose()

    # 5. Close Redis connections
    if hasattr(app.state, "redis_client"):
        app.state.redis_client.close()

    logger.info("Graceful shutdown completed")
    shutdown_event.set()

def signal_handler(signum, frame):
    logger.info(f"Received signal {signum}")
    asyncio.create_task(graceful_shutdown())

# Register signal handlers
signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# worker.py - Graceful shutdown for worker
async def worker_shutdown():
    logger.info("Worker graceful shutdown...")

    # 1. Stop picking up new tasks
    engine.stop_accepting_tasks()

    # 2. Wait for current message generation to complete
    await engine.wait_for_completion(timeout=60)

    # 3. Cleanup
    await engine.cleanup()

    logger.info("Worker shutdown complete")
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Zero dropped requests during rolling deployment
- [ ] Shutdown time < 30s

---

### 3.3 Security Hardening (Hafta 7)

#### Task 3.3.1: Secret Management (HashiCorp Vault / AWS Secrets Manager)
**Neden gerekli**: `.env` file production'da g√ºvenli deƒüil; centralized secret management.

**Implementation**:
```python
# backend/secrets.py - Vault integration
import hvac

class SecretManager:
    def __init__(self):
        self.vault_client = None
        if os.getenv("VAULT_ADDR"):
            self.vault_client = hvac.Client(
                url=os.getenv("VAULT_ADDR"),
                token=os.getenv("VAULT_TOKEN")
            )

    def get_secret(self, path: str, key: str) -> str:
        """Get secret from Vault or fallback to env"""
        if self.vault_client:
            try:
                secret = self.vault_client.secrets.kv.v2.read_secret_version(path)
                return secret["data"]["data"][key]
            except Exception as e:
                logger.warning(f"Vault fetch failed: {e}, falling back to env")

        # Fallback to environment variable
        return os.getenv(key)

# Usage
secret_mgr = SecretManager()
OPENAI_API_KEY = secret_mgr.get_secret("piyasa-chatbot/prod", "OPENAI_API_KEY")
DB_PASSWORD = secret_mgr.get_secret("piyasa-chatbot/prod", "DB_PASSWORD")
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Zero secrets in version control
- [ ] Secrets rotation policy implemented

---

#### Task 3.3.2: Rate Limiting (Per-Bot, Per-User, Per-IP)
**Neden gerekli**: Global rate limiting yetersiz; granular control gerekli.

**Implementation**:
```python
# backend/middleware/rate_limiter.py
from fastapi import Request, HTTPException
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(
    key_func=get_remote_address,
    storage_uri=os.getenv("REDIS_URL"),
    strategy="fixed-window"  # or "moving-window"
)

# Per-IP rate limiting
@app.get("/api/resource")
@limiter.limit("100/minute")  # 100 requests per minute per IP
async def some_endpoint(request: Request):
    return {"ok": True}

# Per-user rate limiting
def get_user_id(request: Request):
    return request.state.user.username if hasattr(request.state, "user") else "anonymous"

@app.post("/bots")
@limiter.limit("10/minute", key_func=get_user_id)  # 10 bot creations per minute per user
async def create_bot(request: Request, bot: BotCreate):
    pass

# Per-bot message generation rate limiting (in behavior engine)
class BehaviorEngine:
    async def check_bot_rate_limit(self, bot_id: int) -> bool:
        """Check if bot exceeded hourly message limit"""
        cache_key = f"bot:{bot_id}:hourly_messages"
        count = int(self.redis.get(cache_key) or 0)

        limit = self.settings.get("bot_hourly_msg_limit", {}).get("max", 12)

        if count >= limit:
            logger.warning(f"Bot {bot_id} exceeded hourly limit: {count}/{limit}")
            return False

        # Increment counter (expire after 1 hour)
        self.redis.incr(cache_key)
        self.redis.expire(cache_key, 3600)

        return True
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Rate limit coverage: 100% of write endpoints
- [ ] Rate limit bypass attempts logged

---

## üìÖ PHASE 4: DEVOPS & AUTOMATION
**S√ºre**: 1-2 hafta
**√ñncelik**: P1 (Y√ºksek)
**Hedef**: Automated testing, deployment, operational excellence

### 4.1 CI/CD Pipeline (Hafta 7-8)

#### Task 4.1.1: GitHub Actions Workflows
**Neden gerekli**: Manual testing/deployment error-prone; automated pipelines.

**Implementation**:
```yaml
# .github/workflows/ci.yml - Continuous Integration
name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test-backend:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: app
          POSTGRES_PASSWORD: app
          POSTGRES_DB: app_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio

    - name: Run type checking
      run: mypy backend/ --strict

    - name: Run tests with coverage
      env:
        DATABASE_URL: postgresql+psycopg://app:app@localhost:5432/app_test
        REDIS_URL: redis://localhost:6379/0
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        pytest tests/ \
          --cov=backend \
          --cov-report=xml \
          --cov-report=html \
          --junitxml=test-results.xml \
          -v

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

    - name: Upload test results
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: test-results.xml

  test-frontend:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'

    - name: Cache node modules
      uses: actions/cache@v3
      with:
        path: node_modules
        key: ${{ runner.os }}-node-${{ hashFiles('package-lock.json') }}

    - name: Install dependencies
      run: npm ci

    - name: Run tests
      run: npm test -- --coverage

    - name: Build
      run: npm run build

  lint:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Run Python linters
      run: |
        pip install ruff black isort
        ruff check backend/
        black --check backend/
        isort --check backend/

    - name: Run security checks
      run: |
        pip install bandit safety
        bandit -r backend/
        safety check

  docker-build:
    runs-on: ubuntu-latest
    needs: [test-backend, test-frontend]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Build API image
      uses: docker/build-push-action@v4
      with:
        context: .
        file: ./Dockerfile.api
        push: false
        tags: piyasa-chatbot-api:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build Frontend image
      uses: docker/build-push-action@v4
      with:
        context: .
        file: ./Dockerfile.frontend
        push: false
        tags: piyasa-chatbot-frontend:${{ github.sha }}
```

```yaml
# .github/workflows/cd.yml - Continuous Deployment
name: CD Pipeline

on:
  push:
    branches: [main]
    tags:
      - 'v*'

jobs:
  deploy-staging:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v3

    - name: Deploy to staging
      env:
        KUBE_CONFIG: ${{ secrets.KUBE_CONFIG_STAGING }}
      run: |
        echo "$KUBE_CONFIG" > kubeconfig
        kubectl --kubeconfig=kubeconfig apply -f k8s/staging/

  deploy-production:
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')

    steps:
    - uses: actions/checkout@v3

    - name: Deploy to production
      env:
        KUBE_CONFIG: ${{ secrets.KUBE_CONFIG_PROD }}
      run: |
        echo "$KUBE_CONFIG" > kubeconfig
        kubectl --kubeconfig=kubeconfig apply -f k8s/production/
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] CI pipeline run time < 10 minutes
- [ ] Zero manual deployments to production

---

#### Task 4.1.2: Database Migrations (Alembic)
**Neden gerekli**: Manual schema changes error-prone; versioned migrations.

**Implementation**:
```bash
# Install Alembic
pip install alembic

# Initialize Alembic
alembic init alembic

# alembic/env.py - Configure SQLAlchemy models
from database import Base
target_metadata = Base.metadata

# Create first migration
alembic revision --autogenerate -m "Initial schema"

# Apply migration
alembic upgrade head

# Rollback migration
alembic downgrade -1
```

```python
# alembic/versions/001_add_bot_memory_table.py
"""Add bot memory table

Revision ID: 001
Revises:
Create Date: 2025-10-27
"""
from alembic import op
import sqlalchemy as sa

def upgrade():
    op.create_table(
        'bot_memories',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('bot_id', sa.Integer(), sa.ForeignKey('bots.id'), nullable=False),
        sa.Column('memory_type', sa.String(32), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('relevance_score', sa.Float(), default=1.0),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('last_used_at', sa.DateTime(), nullable=False)
    )

    op.create_index('ix_memories_bot_type', 'bot_memories', ['bot_id', 'memory_type'])

def downgrade():
    op.drop_index('ix_memories_bot_type')
    op.drop_table('bot_memories')
```

**CI Integration**:
```yaml
# .github/workflows/ci.yml - Add migration check
    - name: Check migrations
      run: |
        alembic check  # Verify no pending migrations
        alembic upgrade head  # Apply migrations in test DB
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] 100% schema changes via migrations
- [ ] Zero manual SQL scripts in production

---

### 4.2 Kubernetes Deployment (Hafta 8)

#### Task 4.2.1: Kubernetes Manifests
**Neden gerekli**: Docker Compose production i√ßin yetersiz; K8s orchestration.

**Implementation**:
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: piyasa-chatbot

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: piyasa-chatbot-config
  namespace: piyasa-chatbot
data:
  LOG_LEVEL: "INFO"
  LLM_PROVIDER: "openai"
  LLM_MODEL: "gpt-4o-mini"

---
# k8s/secret.yaml (use Sealed Secrets or external secret operator in production)
apiVersion: v1
kind: Secret
metadata:
  name: piyasa-chatbot-secrets
  namespace: piyasa-chatbot
type: Opaque
stringData:
  OPENAI_API_KEY: "your-openai-key"
  DB_PASSWORD: "your-db-password"
  TOKEN_ENCRYPTION_KEY: "your-encryption-key"

---
# k8s/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: piyasa-chatbot
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: api
        image: your-registry/piyasa-chatbot-api:latest
        ports:
        - containerPort: 8000
        envFrom:
        - configMapRef:
            name: piyasa-chatbot-config
        - secretRef:
            name: piyasa-chatbot-secrets
        env:
        - name: DATABASE_URL
          value: "postgresql+psycopg://$(DB_USER):$(DB_PASSWORD)@postgres:5432/app"
        - name: REDIS_URL
          value: "redis://redis:6379/0"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5

---
# k8s/api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: api
  namespace: piyasa-chatbot
spec:
  selector:
    app: api
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  type: ClusterIP

---
# k8s/worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker
  namespace: piyasa-chatbot
spec:
  replicas: 4  # Scale workers based on load
  selector:
    matchLabels:
      app: worker
  template:
    metadata:
      labels:
        app: worker
    spec:
      containers:
      - name: worker
        image: your-registry/piyasa-chatbot-api:latest
        command: ["python", "worker.py"]
        envFrom:
        - configMapRef:
            name: piyasa-chatbot-config
        - secretRef:
            name: piyasa-chatbot-secrets
        env:
        - name: WORKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name  # Use pod name as worker ID
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

---
# k8s/postgres-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: piyasa-chatbot
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:16-alpine
        env:
        - name: POSTGRES_USER
          value: "app"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: piyasa-chatbot-secrets
              key: DB_PASSWORD
        - name: POSTGRES_DB
          value: "app"
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 20Gi

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: piyasa-chatbot-ingress
  namespace: piyasa-chatbot
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.piyasa-chatbot.com
    secretName: piyasa-chatbot-tls
  rules:
  - host: api.piyasa-chatbot.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api
            port:
              number: 8000

---
# k8s/hpa.yaml - Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: piyasa-chatbot
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**Deployment Commands**:
```bash
# Apply all manifests
kubectl apply -f k8s/

# Check deployment status
kubectl get pods -n piyasa-chatbot
kubectl get svc -n piyasa-chatbot

# View logs
kubectl logs -f deployment/api -n piyasa-chatbot
kubectl logs -f deployment/worker -n piyasa-chatbot

# Scale workers
kubectl scale deployment/worker --replicas=8 -n piyasa-chatbot
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] K8s deployment successful
- [ ] Auto-scaling functional (HPA triggered)
- [ ] Rolling updates zero-downtime

---

### 4.3 Backup & Disaster Recovery (Hafta 8)

#### Task 4.3.1: Database Backup Strategy
**Neden gerekli**: Data loss prevention; disaster recovery.

**Implementation**:
```bash
# backup/pg_backup.sh - Automated PostgreSQL backup
#!/bin/bash
set -e

TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_DIR="/backups/postgres"
BACKUP_FILE="$BACKUP_DIR/piyasa_chatbot_$TIMESTAMP.sql.gz"

# Create backup directory
mkdir -p $BACKUP_DIR

# Dump database
pg_dump -h $DB_HOST -U $DB_USER -d $DB_NAME | gzip > $BACKUP_FILE

# Upload to S3 (or cloud storage)
aws s3 cp $BACKUP_FILE s3://piyasa-chatbot-backups/postgres/

# Keep only last 7 days of local backups
find $BACKUP_DIR -name "*.sql.gz" -mtime +7 -delete

echo "Backup completed: $BACKUP_FILE"

# backup/cronjob.yaml - Kubernetes CronJob for backups
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: piyasa-chatbot
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM UTC
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:16-alpine
            command:
            - /bin/sh
            - -c
            - |
              pg_dump -h postgres -U app -d app | gzip > /backup/backup_$(date +%Y%m%d_%H%M%S).sql.gz
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: piyasa-chatbot-secrets
                  key: DB_PASSWORD
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          restartPolicy: OnFailure
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
```

**Restore Procedure**:
```bash
# restore/pg_restore.sh
#!/bin/bash
BACKUP_FILE=$1

# Download from S3
aws s3 cp s3://piyasa-chatbot-backups/postgres/$BACKUP_FILE /tmp/restore.sql.gz

# Restore
gunzip < /tmp/restore.sql.gz | psql -h $DB_HOST -U $DB_USER -d $DB_NAME

echo "Restore completed from $BACKUP_FILE"
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Daily automated backups
- [ ] Backup retention: 30 days
- [ ] Restore test: successful monthly

---

## üìÖ PHASE 5: ADVANCED FEATURES & OPTIMIZATION
**S√ºre**: 1-2 hafta
**√ñncelik**: P2 (Orta)
**Hedef**: Enhanced capabilities, AI improvements

### 5.1 Advanced AI Features (Hafta 9)

#### Task 5.1.1: Long-term Memory System
**Neden gerekli**: Botlar tutarlƒ± ge√ßmi≈ü hatƒ±rlamalƒ±; persistent personality.

**Implementation**:
```python
# backend/services/memory_service.py
class BotMemoryService:
    """Manage bot's long-term memory"""

    def __init__(self, db: Session, embedding_client: EmbeddingClient):
        self.db = db
        self.embeddings = embedding_client

    async def add_memory(
        self,
        bot_id: int,
        memory_type: str,
        content: str,
        relevance_score: float = 1.0
    ):
        """Add new memory entry"""
        memory = BotMemory(
            bot_id=bot_id,
            memory_type=memory_type,
            content=content,
            relevance_score=relevance_score,
            created_at=datetime.now(timezone.utc),
            last_used_at=datetime.now(timezone.utc)
        )
        self.db.add(memory)
        self.db.commit()

        # Store embedding for semantic search
        embedding = await self.embeddings.embed(content)
        await self._store_embedding(memory.id, embedding)

    async def recall_memories(
        self,
        bot_id: int,
        query: str,
        limit: int = 5
    ) -> List[BotMemory]:
        """Recall relevant memories via semantic search"""

        # Get query embedding
        query_embedding = await self.embeddings.embed(query)

        # Find similar memories (cosine similarity)
        memories = self.db.query(BotMemory).filter(
            BotMemory.bot_id == bot_id
        ).all()

        # Rank by similarity
        scored_memories = []
        for memory in memories:
            memory_embedding = await self._get_embedding(memory.id)
            similarity = cosine_similarity(query_embedding, memory_embedding)
            scored_memories.append((memory, similarity))

        # Sort by relevance
        scored_memories.sort(key=lambda x: x[1], reverse=True)

        # Update last_used_at for recalled memories
        for memory, _ in scored_memories[:limit]:
            memory.last_used_at = datetime.now(timezone.utc)
            memory.usage_count += 1

        self.db.commit()

        return [m for m, _ in scored_memories[:limit]]

    async def forget_low_relevance(self, bot_id: int, threshold: float = 0.3):
        """Garbage collect low-relevance memories"""
        self.db.query(BotMemory).filter(
            BotMemory.bot_id == bot_id,
            BotMemory.relevance_score < threshold
        ).delete()
        self.db.commit()

# behavior_engine.py - Use memory in prompt
async def build_prompt_with_memory(bot: Bot, chat: Chat, topic: str):
    memory_service = BotMemoryService(db, embedding_client)

    # Recall relevant memories
    memories = await memory_service.recall_memories(
        bot.id,
        query=f"Topic: {topic}",
        limit=3
    )

    # Inject memories into prompt
    memory_context = "\n".join([
        f"- {m.content} ({m.memory_type})"
        for m in memories
    ])

    prompt = f"""
Sen {bot.name} adlƒ± bir Telegram kullanƒ±cƒ±sƒ±sƒ±n.

## Ge√ßmi≈ü Hatƒ±ralarƒ±m:
{memory_context}

## ≈ûu anki sohbet:
...
"""

    return prompt
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Memory recall accuracy > 80%
- [ ] Personality consistency score > 90%

---

#### Task 5.1.2: Multi-Agent Interaction Strategy
**Neden gerekli**: Botlar birbirleriyle etkile≈üim kurmalƒ±; realistic conversation dynamics.

**Implementation**:
```python
# behavior_engine/interaction_strategy.py
class MultiAgentInteractionStrategy:
    """Coordinate multi-bot interactions"""

    def __init__(self, db: Session):
        self.db = db

    async def select_reply_target(
        self,
        bot: Bot,
        chat: Chat,
        recent_messages: List[Message]
    ) -> Optional[Message]:
        """Intelligently select which message to reply to"""

        # 1. Prioritize mentions
        mentions = [m for m in recent_messages if f"@{bot.username}" in m.text]
        if mentions:
            return mentions[0]

        # 2. Prioritize messages from bots with relationship
        relationships = await self._get_bot_relationships(bot.id)
        for msg in recent_messages:
            if msg.bot_id in relationships:
                return msg

        # 3. Prioritize controversial stances
        for msg in recent_messages:
            if await self._is_controversial(bot, msg):
                return msg

        # 4. Random recent message
        return random.choice(recent_messages[:5]) if recent_messages else None

    async def _get_bot_relationships(self, bot_id: int) -> Dict[int, str]:
        """Get bot's relationships (friend, rival, neutral)"""
        # Stored in bot_memories with type='relationship'
        relationships = self.db.query(BotMemory).filter(
            BotMemory.bot_id == bot_id,
            BotMemory.memory_type == "relationship"
        ).all()

        # Parse relationships (e.g., "@AliTrader is my rival")
        # Return dict: {target_bot_id: relationship_type}
        return {}  # Implementation details

    async def _is_controversial(self, bot: Bot, message: Message) -> bool:
        """Check if message contradicts bot's stances"""
        # Compare message sentiment/topic with bot's stances
        # Return True if significant disagreement
        return False  # Implementation details
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] Reply targeting accuracy > 75%
- [ ] Conversation coherence score > 80%

---

### 5.2 Performance Fine-tuning (Hafta 10)

#### Task 5.2.1: Database Query Profiling & Optimization
**Neden gerekli**: Identify slow queries; optimize with indexes/materialized views.

**Implementation**:
```python
# scripts/profile_queries.py - Query profiler
import cProfile
import pstats
from sqlalchemy import event
from sqlalchemy.engine import Engine
import time

# Log slow queries
@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop(-1)
    if total > 0.1:  # Log queries > 100ms
        logger.warning(
            "Slow query detected",
            extra={
                "extra_fields": {
                    "duration_ms": total * 1000,
                    "query": statement[:200],
                    "parameters": parameters
                }
            }
        )

# Run load test with profiling
cProfile.run('run_load_test()', 'profile_output')
p = pstats.Stats('profile_output')
p.sort_stats('cumulative').print_stats(20)
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] p99 query latency < 50ms
- [ ] Zero queries > 500ms

---

#### Task 5.2.2: LLM Response Caching
**Neden gerekli**: Duplicate prompts waste tokens/latency; cache responses.

**Implementation**:
```python
# backend/caching/llm_cache.py
import hashlib

class LLMResponseCache:
    def __init__(self, redis_client: redis.Redis, ttl: int = 3600):
        self.redis = redis_client
        self.ttl = ttl

    def _hash_prompt(self, prompt: str, model: str) -> str:
        """Generate cache key from prompt"""
        return hashlib.sha256(f"{model}:{prompt}".encode()).hexdigest()

    def get(self, prompt: str, model: str) -> Optional[str]:
        """Get cached response"""
        cache_key = f"llm_cache:{self._hash_prompt(prompt, model)}"
        cached = self.redis.get(cache_key)
        if cached:
            logger.debug(f"LLM cache hit: {cache_key}")
            return cached
        return None

    def set(self, prompt: str, model: str, response: str):
        """Cache response"""
        cache_key = f"llm_cache:{self._hash_prompt(prompt, model)}"
        self.redis.setex(cache_key, self.ttl, response)
        logger.debug(f"LLM cache set: {cache_key}")

# llm_client.py - Use cache
llm_cache = LLMResponseCache(redis_client)

async def generate_message(prompt: str, model: str) -> str:
    # Check cache first
    cached_response = llm_cache.get(prompt, model)
    if cached_response:
        return cached_response

    # Generate from LLM
    response = await llm_client.chat_completion(prompt, model)

    # Cache response
    llm_cache.set(prompt, model, response)

    return response
```

**√ñl√ß√ºlebilir Hedef**:
- [ ] LLM cache hit rate > 20%
- [ ] Token cost reduction > 15%

---

## üéØ SUCCESS METRICS

### Overall Project Goals
- [ ] **Performance**: 200 bot @ 10 msg/hour with avg latency < 5s
- [ ] **Uptime**: 99.5% uptime in production
- [ ] **Code Quality**: Test coverage > 80%, zero critical bugs
- [ ] **Scalability**: Linear scaling up to 8 workers
- [ ] **Security**: Zero security vulnerabilities (Bandit, Safety scans)
- [ ] **Operations**: Zero manual deployments, automated backups

### Phase-wise Success Criteria

**PHASE 1 (Performance)**:
- Database query latency p99 < 50ms
- Message generation throughput > 20/sec (multi-worker)
- Cache hit rates: Bot profiles >80%, History >90%
- Load test: 100 bots @ 10msg/hr sustained 1 hour

**PHASE 2 (Architecture)**:
- Code complexity: All modules < 300 lines
- Type coverage > 95%
- Dependency injection > 90%
- Exception handling standardized

**PHASE 3 (Production-Ready)**:
- Monitoring: 20+ metrics exported to Prometheus
- Health checks: <100ms latency
- Tracing: 80% coverage of critical paths
- Security: Secrets in Vault, rate limiting enforced

**PHASE 4 (DevOps)**:
- CI pipeline: <10 min runtime, 100% automated
- K8s deployment: Zero-downtime rolling updates
- Backups: Daily automated, monthly restore test

**PHASE 5 (Advanced)**:
- Memory recall accuracy > 80%
- LLM cache hit rate > 20%
- Bot interaction coherence > 80%

---

## üìö DELIVERABLES

### Documentation
- [ ] `docs/architecture_v2.md` - Updated architecture diagram
- [ ] `docs/api_spec.md` - OpenAPI/Swagger spec
- [ ] `docs/deployment_guide.md` - K8s deployment instructions
- [ ] `docs/operations_runbook.md` - Incident response procedures
- [ ] `docs/performance_benchmarks.md` - Load test results
- [ ] `docs/database_schema.md` - ER diagrams + migration guide

### Code Artifacts
- [ ] Modularized codebase (backend/, behavior_engine/)
- [ ] Comprehensive test suite (80%+ coverage)
- [ ] CI/CD pipelines (GitHub Actions)
- [ ] K8s manifests (production-ready)
- [ ] Monitoring dashboards (Grafana JSON exports)

### Operational Tools
- [ ] Load testing scripts (`tests/load_test.py`)
- [ ] Backup/restore scripts (`backup/`, `restore/`)
- [ ] Metrics exporter (Prometheus)
- [ ] Alert rules (Prometheus AlertManager)

---

## üö¶ RISK MANAGEMENT

### Technical Risks

**Risk 1: Database Performance Degradation**
- **Impact**: High (System slow/unresponsive)
- **Mitigation**: Phase 1 Task 1.1 (Index optimization), Load testing
- **Contingency**: Read replicas, PostgreSQL query optimization

**Risk 2: LLM API Rate Limiting**
- **Impact**: High (Message generation blocked)
- **Mitigation**: Circuit breakers, retry policies, multi-LLM fallback
- **Contingency**: Queue messages, burst credits, alternative providers (Groq)

**Risk 3: Deployment Downtime**
- **Impact**: Medium (Service interruption)
- **Mitigation**: K8s rolling updates, health checks, blue-green deployment
- **Contingency**: Rollback procedure, database backup restore

**Risk 4: Secret Exposure**
- **Impact**: Critical (Security breach)
- **Mitigation**: Vault integration, secret scanning (git-secrets), audit logs
- **Contingency**: Immediate key rotation, incident response

### Schedule Risks

**Risk 5: Phase Overrun**
- **Impact**: Medium (Delayed launch)
- **Mitigation**: Weekly checkpoints, MVP scope definition, parallel work
- **Contingency**: Phase prioritization, reduce P2 scope

---

## üîÑ NEXT STEPS

### Immediate Actions (Week 1)
1. **Kickoff Meeting**: Review roadmap, assign responsibilities
2. **Environment Setup**: Staging K8s cluster, monitoring stack
3. **Database Baseline**: Run current load test, collect metrics
4. **Phase 1 Start**: Task 1.1.1 (Database index audit)

### Weekly Cadence
- **Monday**: Sprint planning, task assignment
- **Wednesday**: Mid-week sync, blocker resolution
- **Friday**: Demo, retrospective, metrics review

### Stakeholder Communication
- **Weekly**: Progress report (completed tasks, metrics, blockers)
- **Bi-weekly**: Demo session (show working features)
- **Monthly**: Roadmap review (adjust priorities based on feedback)

---

## üìû SUPPORT & ESCALATION

Ben (Claude Code) bu roadmap'i olu≈ütururken enterprise production sistemleri i√ßin industry best practices uyguladƒ±m. Her task i√ßin:
- **Why (Neden)**: ƒ∞≈ü deƒüeri ve teknik gerek√ße
- **What (Ne)**: A√ßƒ±k implementasyon detaylarƒ±
- **How (Nasƒ±l)**: Code snippets ve configuration examples
- **Success Criteria (Ba≈üarƒ± Kriterleri)**: √ñl√ß√ºlebilir hedefler

Sorularƒ±nƒ±z olursa:
1. **Teknik Detaylar**: Her task i√ßin √∂rnek kod/config var, sorular i√ßin detaylandƒ±rabilirim
2. **√ñnceliklendirme**: ƒ∞≈ü hedefleriniz deƒüi≈üirse roadmap'i revize edebiliriz
3. **Implementation Guidance**: Adƒ±m adƒ±m her task'ƒ± birlikte implement edebiliriz

**Hazƒ±r mƒ±sƒ±nƒ±z?**
≈ûimdi PHASE 1 Task 1.1.1 ile ba≈ülayalƒ±m: Database Index Audit & Optimization üöÄ

---

*Generated with expertise by Claude Code - Production-Ready Enterprise Architect*
*Version: 2.0.0 - Professional Upgrade Roadmap*
*Date: 2025-10-27*
